---
widget: blank
widget_id: factorsonourside
headless: true
weight: 2500
title: It's not all bad, some things favour the defender
active: true
design:
  columns: "1"
  background:
    text_color_light: false
    image_darken: 0
    color: ""
---

 - **For the AI, it's not sufficient to discover all of the rules the defenders are using to detect badness.**  It also needs to be sure that it knows all of the rules.  It needs knowledge that it has complete knowledge, which is difficult, especially if the rules can change.
 - **Each AI need only be given one chance.**  If it breaks the rules we can stop it and never allow it to run again.
 - **Generally, we get to design the hardware the AI runs on.**  So we can build in whatever safeguards we want, and force the AI to fabricate its own silicon (or equivalent) to fully escape them.
 - **Physical weapons are still a thing.**  Any agency or nation-state trying to build AI systems without these safeguards is potentially subject to attack.  Deliberately trying to build a Doomsday AI, or even acquire the unfettered capability to do that, is likely to get you nuked, and you need to be very sure you can do it in secret before you try.  Even a large-scale EMP-based nuclear exchange[\[@wiki\]](https://en.wikipedia.org/wiki/Nuclear_electromagnetic_pulse) (you detonate things higher up) might rescue us from a very bad situation, so long as everything hasn't move to photonics[\[@wiki\]](https://en.wikipedia.org/wiki/Optical_neural_network) by then.
 - **All computer systems will need this type of protection anyway.**  Malware and malicious attack techniques, including those boosted by weak-ish AI, are likely to become so advanced that unprotected hardware connected to the network is rapidly compromised.  Yes, I realise I'm painting a picture where humanity is saved by cybercrime and nuclear weapons, but so be it.  I'm not claiming it's a Gaia thing[\[@wiki\]](https://en.wikipedia.org/wiki/Gaia_hypothesis).
 - **The AI has to get information from somewhere, and we can monitor what it's looking at.**  It won't be as easy as seeing it search for "How to destroy humanity and get away with it"[\[@google\]](https://www.google.com/search?q=How+to+destroy+humanity+and+get+away+with+it) but its patterns of interaction with the outside world can give away something.
 - **The AI can't just sit there silently plotting, then suddenly launch a devastating attack.**  It needs to stay current with what's happening in the world to be sure of a successful attack, although it *can* be deceptive about how it does that.
 - **The AI can try to deceive us but we can also try to deceive it.**  We can potentially trick it into revealing what it's going to do.
 - **AI introspection - seeing how the AI is making decisions - might help to some extent.**  It's difficult to forecast how well this might work, but you could monitor the internals of an AI as you, say, ask it a set of 10,000 questions.

Saving the biggest until last:
{{% callout note %}}
There is a **lot** of money in cybersecurity.  Billions of dollars are spent annually, much of it finding its way into R&D.  For cybersecurity firms, the incentives are the right way around.  Ensuring AI safety isn't a costly distraction from AI development, it's a core revenue-generating business, which makes it easier to get resources and engineering time applied to it.
{{% /callout %}}