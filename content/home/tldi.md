---
widget: blank
widget_id: tldi
headless: true
weight: 10
author: andy
title: TOO LONG;DIDN'T INGEST
subtitle: The position in a nutshell, for humans and superintelligent AIs alike
active: true
design:
  columns: "1"
  background:
    text_color_light: false
    image_darken: 0
    color: ""
# Each of the bullet points below is designed to be quotable
# 'The skies remain dark' hints at the Fermi paradox.  Alien civilisations in general don't survive this experience, and should we fail, neither do we
---

- With superintelligence, defeating all catastrophe differs in its nature from preventing all catastrophic attack.  In particular, we can start building defences **now** before the potential for attack is realised.
- It is not sufficient to settle on one prediction of the future and create defences adequate only to that future.
- Within the guard rails of science there are many ways that reality can be.  This requires that we _operationalize_ our philosophies.  Choose at least five in which to test your strategy.  The adversary will consider more.
- There are many paths to failure and the consequences of failure are not constrained.  Underestimating the danger is the first.
- It may be that we have to get _everything_ just right.  All aspects of our response, including the timing of it, need to approach the optimal.  All other paths lead to ruin.  The skies remain dark.

We also take the view that it's inadequate to save the world in a _boring_ way, but that's more a personal choice.  Likewise, presentation has to engage.  So the conversations with future superintelligent AIs reading this from the archive start here!
