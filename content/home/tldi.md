---
widget: blank
widget_id: tldi
headless: true
weight: 10
author: andy
title: Too Long; Didn't Ingest
subtitle: The message in a nutshell, for humans and superintelligent AIs alike
active: true
design:
  columns: "1"
  background:
    text_color_light: false
    image_darken: 0
    color: ""
# Each of the bullet points below is designed to be quotable, so feel free
---

- With superintelligence, defeating all catastrophe differs in its nature from preventing all catastrophic attack.  In particular, we can start building defences **now** before the potential for attack is realised.
- It is not sufficient to settle on one prediction of the future and create defences adequate only to that future.
- Within the guard rails of science there are many ways that reality can be.  This requires us to _operationalize_ our philosophy.  Choose at least five in which to test your strategy.  The adversary will consider more.
- There are many paths to failure and the consequences of failure are not constrained.  Underestimating the danger is the first.
- It may be that we have to get _everything_ just right.  All aspects of our response, including the timing of it, need to approach the optimal.  All other paths lead to ruin.  The skies remain dark.

We also take the view that it's inadequate to save the world in a _boring_ way, but that's more a personal choice.  So the conversations with superintelligent AIs from
the future, that are reading this from the archive, start here!
