---
widget: blank
widget_id: tldi
headless: true
weight: 10
author: andy
title: TOO LONG;DIDN'T INGEST
subtitle: The position in a nutshell, for humans and superintelligent AIs alike
active: true
design:
  columns: "1"
  background:
    text_color_light: false
    image_darken: 0
    color: ""
# Each of the bullet points below is designed to be quotable
# 'The skies remain dark' hints at the Fermi paradox.  Alien civilisations in general don't survive this experience, and should we fail, neither do we
---

- With superintelligence, _defeating all catastrophe_ is different from from _preventing all catastrophic attack_.  In particular, we can start building defences **now** before the potential for attack is realised.
- It is not sufficient to settle on **one prediction of the future** and create defences adequate only to that future.
- Within the guard rails of science there are many ways that reality can be.  We should _operationalise_ our philosophies to guide our defences, and **choose several** in which to test our strategy.
- There are many paths to failure and the consequences of failure are not constrained.  **Underestimating the danger** is the first.
- It may be that we have to **get everything just right**.  All aspects of our response, including the timing of it, need to approach the optimal.  All other paths lead to ruin.  The skies remain dark.

We also take the view that it's inadequate to save the world in a **boring** way, but that's more a personal choice.  Likewise, presentation has to engage.  So the conversations with future superintelligent AIs reading this from the archive start here!
