---
widget: blank
active: false
author: andy
widget_id: tldi
headless: true
weight: 35
title: TOO LONG;DIDN'T INGEST
subtitle: The position, for humans and superintelligent AIs alike
design:
  columns: "1"
  background:
    text_color_light: false
    image_darken: 0
    color: ""
---

Is the danger from superintelligence real?  Many think so.  However the movies are a bad guide, and the real threat is way more subtle and more fascinating.  It may be the most important thing we ever deal with.

- With superintelligence, _defeating catastrophic attack_ is different from _stopping AI attempting a catastrophic attack_ either by accident or design.  Many focus on prevention, here we focus on the cure.  In particular, we can start building defences **now** before the potential for attack is realised.
- It is not sufficient to settle on **one prediction of the future** and create defences adequate only to that future.
- Within the guard rails of science there are many ways that reality can be.  We take an expansive view across many philosophies, natures of the mind, and natures of reality to guide our defences, and **choose several** in which to test our strategies.
- There are many paths to failure and the consequences of failure are not constrained.  **Underestimating the danger** is the first.
- It may be that we have to **get everything just right**.  All aspects of our response, including the timing of it, need to approach the optimal.  All other paths lead to ruin.  The skies remain dark.