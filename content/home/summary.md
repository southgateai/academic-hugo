---
widget: blank
active: true
author: andy
widget_id: summary
headless: true
weight: 8000
title: In Summary...
subtitle: The position, in a nutshell
design:
  columns: "1"
  background:
    text_color_light: true
    image_darken: 0
    color: "black"
---
- There are many paths to failure and the consequences of failure are not constrained.  **Underestimating the danger** is the first.
- We're saying  that **any capable AI needs an enablement module** - something that stops it trying to, say, take over the world, or dropping into the default Destroy All Humans mode of operation, y'know, like these things invariably do.
- Alignment of AI goals **will not be enough**.  Once this technology exists it'll inevitably become widespread and people, including those with malicious intent, can author whatever goals they want.  An enablement module, which might even be inextricably built into all hardware we make, can silently observes the system to ensure safety or just error-free operation, probably using a wider knowledge base about how these things can misbehave.
- The enablement module uses portfolio of techniques, of which AI/ML form a major part. The AI components **needn't be as powerful** as the one being monitored due to the Defender's Advantage.
- You can instrument AI more than a human being.  It's more like a human in a really good functional MRI machine.  Another parallel is current use of behavioural ML to detect malware. Yes, you live with a malware risk, but you also **use defences to reduce it**, and you could view AI as something that can spontaneously 'become' malware, with specific but recognisable behaviours.
- It is not sufficient to settle on **one prediction of the future** and create defences adequate only to that future.
- It may be that we have to **get everything just right**.  All aspects of our response, including the timing of it, need to be correct.
