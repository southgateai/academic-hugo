---
widget: blank
active: true
author: andy
widget_id: content
headless: true
weight: 1200
title: What's here?
subtitle:
design:
  columns: "1"
  background:
    text_color_light: false
    image_darken: 0
    color: ""
---
This site talks about the **defensive** engineering we need to do to keep AI safe (that's in the dark-background panels), and the interesting questions we'll meet along the way.  The dark panels don't depend on the answers to the light panels.

What we don't have here is a detailed description of the threat - for that, you need Eliezer's fabulous [AGI (Artificial General Intelligence) Ruin: A List of Lethalities](https://www.alignmentforum.org/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities).  This site is exploring whether point 31 there is as clear-cut as it seems.  We also don't talk much about [AI alignment](https://en.wikipedia.org/wiki/AI_alignment), that's been covered pretty comprehensively elsewhere and does not entirely get us off the hook.
