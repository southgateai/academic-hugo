---
widget: blank
active: true
author: andy
widget_id: content
headless: true
weight: 1200
title: What's here?
subtitle:
design:
  columns: "1"
  background:
    text_color_light: false
    image_darken: 0
    color: ""
---
This site talks about the **defensive** engineering we need to do to keep AI safe.

If you want a commercial angle, try the [Industry](/industry) page.  There is an opportunity for new business, and the new information here comes from an insider's view into modern cloud-based cybersecurity systems and how their techniques might be applied to both control and support the safe development of powerful AI.

AI Doomer[\[@wiki\]](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence)?  Head over to the [Defensive Argument](/defensive) page.

What we don't have here is a detailed description of the threat - for that, you need Eliezer's fabulous [AGI (Artificial General Intelligence) Ruin: A List of Lethalities](https://www.alignmentforum.org/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities).  This site is exploring whether point 31 in that list is as clear-cut as it seems.  We also don't talk much about [AI alignment](https://en.wikipedia.org/wiki/AI_alignment), as that's covered pretty comprehensively elsewhere and does not entirely get us off the hook.
